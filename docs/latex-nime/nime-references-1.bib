
@inproceedings{brown_understanding_2018,
	address = {Genoa Italy},
	title = {Understanding {User}-{Defined} {Mapping} {Design} in {Mid}-{Air} {Musical} {Performance}},
	isbn = {978-1-4503-6504-8},
	url = {https://dl.acm.org/doi/10.1145/3212721.3212810},
	doi = {10.1145/3212721.3212810},
	abstract = {Modern gestural interaction and motion capture technology is frequently incorporated into Digital Musical Instruments (DMIs) to enable new methods of musical expression. A major topic of interest in this domain concerns how a performer’s actions are linked to the production of sound. Some DMI developers choose to design these mapping strategies themselves, while others expose this design space to performers. This work explores the latter of these scenarios, studying the user-defined mapping strategies of a group of experienced mid-air musicians chosen from a rare community of DMI practitioners. Participants are asked to design mappings for a piece of music to determine what factors influence their choices. The findings reveal novice performers spend little time reviewing mapping choices, more time practising, and design mappings that adhere to musical metaphors. Experienced performers edit mappings continuously and focus on the ergonomics of their mapping designs.},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Movement} and {Computing}},
	publisher = {ACM},
	author = {Brown, Dom and Nash, Chris and Mitchell, Tom},
	month = jun,
	year = {2018},
	pages = {1--8},
	file = {Brown et al. - 2018 - Understanding User-Defined Mapping Design in Mid-A.pdf:/Users/admin/Zotero/storage/YIH3YUMV/Brown et al. - 2018 - Understanding User-Defined Mapping Design in Mid-A.pdf:application/pdf},
}


@article{fiebrink_meta-instrument_2009,
	title = {A {Meta}-{Instrument} for {Interactive}, {On}-the-{Fly} {Machine} {Learning}},
	abstract = {Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance.},
	language = {en},
	author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R},
	year = {2009},
	file = {Fiebrink et al. - A Meta-Instrument for Interactive, On-the-Fly Mach.pdf:/Users/admin/Zotero/storage/5IKD2X4J/Fiebrink et al. - A Meta-Instrument for Interactive, On-the-Fly Mach.pdf:application/pdf},
}

@INPROCEEDINGS{bullock:mllib,
	AUTHOR = "James Bullock and Ali Momeni",
	TITLE = "ml.lib: Robust, Cross-platform, Open-source Machine Learning for Max and Pure Data",
	BOOKTITLE = "Proceedings of the International Conference on New Interfaces for Musical Expression",
	YEAR = {2015}
}

@incollection{escalera_gesture_2017,
	address = {Cham},
	title = {The {Gesture} {Recognition} {Toolkit}},
	isbn = {978-3-319-57020-4 978-3-319-57021-1},
	url = {http://link.springer.com/10.1007/978-3-319-57021-1_17},
	abstract = {The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting ﬂexibility and customization for advanced users. The toolkit features a broad range of classiﬁcation and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting.},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Gesture {Recognition}},
	publisher = {Springer International Publishing},
	author = {Gillian, Nicholas and Paradiso, Joseph A.},
	editor = {Escalera, Sergio and Guyon, Isabelle and Athitsos, Vassilis},
	year = {2017},
	doi = {10.1007/978-3-319-57021-1_17},
	note = {Series Title: The Springer Series on Challenges in Machine Learning},
	pages = {497--502},
	file = {Gillian and Paradiso - 2017 - The Gesture Recognition Toolkit.pdf:/Users/admin/Zotero/storage/6FDCFVAQ/Gillian and Paradiso - 2017 - The Gesture Recognition Toolkit.pdf:application/pdf},
}




@article{jourdan_machine_2023,
	title = {Machine {Learning} for {Musical} {Expression}: {A} {Systematic} {Literature} {Review}},
	abstract = {For several decades NIME community has always been appropriating machine learning (ML) to apply for various tasks such as gesture-sound mapping or sound synthesis for digital musical instruments. Recently, the use of ML methods seems to have increased and the objectives have diversified. Despite its increasing use, few contributions have studied what constitutes the culture of learning technologies for this specific practice. This paper presents an analysis of 69 contributions selected from a systematic review of the NIME conference over the last 10 years. This paper aims at analysing the practices involving ML in terms of the techniques and the task used and the ways to interact this technology. It thus contributes to a deeper understanding of the specific goals and motivation in using ML for musical expression. This study allows us to propose new perspectives in the practice of these techniques.},
	language = {en},
	author = {Jourdan, Théo and Caramiaux, Baptiste},
	year = {2023},
	file = {Jourdan and Caramiaux - Machine Learning for Musical Expression A Systema.pdf:/Users/admin/Zotero/storage/L7LZDA84/Jourdan and Caramiaux - Machine Learning for Musical Expression A Systema.pdf:application/pdf},
}

@article{fiebrink_reflections_2020,
	title = {Reflections on {Eight} {Years} of {Instrument} {Creation} with {Machine} {Learning}},
	abstract = {Machine learning (ML) has been used to create mappings for digital musical instruments for over twenty-five years, and numerous ML toolkits have been developed for the NIME community. However, little published work has studied how ML has been used in sustained instrument building and performance practices. This paper examines the experiences of instrument builder and performer Laetitia Sonami, who has been using ML to build and refine her Spring Spyre instrument since 2012. Using Sonami’s current practice as a case study, this paper explores the utility, opportunities, and challenges involved in using ML in practice over many years. This paper also reports the perspective of Rebecca Fiebrink, the creator of the Wekinator ML tool used by Sonami, revealing how her work with Sonami has led to changes to the software and to her teaching. This paper thus contributes a deeper understanding of the value of ML for NIME practitioners, and it can inform design considerations for future ML toolkits as well as NIME pedagogy. Further, it provides new perspectives on familiar NIME conversations about mapping strategies, expressivity, and control, informed by a dedicated practice over many years.},
	language = {en},
	author = {Fiebrink, Rebecca and Sonami, Laetitia},
	year = {2020},
	file = {2020 - Proceedings of the International Conference on New.pdf:/Users/admin/Zotero/storage/FYV4EELJ/2020 - Proceedings of the International Conference on New.pdf:application/pdf},
}

@article{caramiaux_machine_2013,
	title = {Machine {Learning} of {Musical} {Gestures}},
	abstract = {We present an overview of machine learning (ML) techniques and their application in interactive music and new digital instrument design. We ﬁrst provide the non-specialist reader an introduction to two ML tasks, classiﬁcation and regression, that are particularly relevant for gestural interaction. We then present a review of the literature in current NIME research that uses ML in musical gesture analysis and gestural sound control. We describe the ways in which machine learning is useful for creating expressive musical interaction, and in turn why live music performance presents a pertinent and challenging use case for machine learning.},
	language = {en},
	author = {Caramiaux, Baptiste and Tanaka, Atau},
	year = {2013},
	file = {Caramiaux and Tanaka - Machine Learning of Musical Gestures.pdf:/Users/admin/Zotero/storage/KXIFY3DI/Caramiaux and Tanaka - Machine Learning of Musical Gestures.pdf:application/pdf},
}



@article{pelinski_pipeline_2023,
	title = {Pipeline for recording datasets and running neural networks on the {Bela} embedded hardware platform},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.11389},
	doi = {10.48550/ARXIV.2306.11389},
	abstract = {Deploying deep learning models on embedded devices is an arduous task: oftentimes, there exist no platform-specific instructions, and compilation times can be considerably large due to the limited computational resources available on-device. Moreover, many music-making applications demand real-time inference. Embedded hardware platforms for audio, such as Bela, offer an entry point for beginners into physical audio computing; however, the need for cross-compilation environments and low-level software development tools for deploying embedded deep learning models imposes high entry barriers on non-expert users. We present a pipeline for deploying neural networks in the Bela embedded hardware platform. In our pipeline, we include a tool to record a multichannel dataset of sensor signals. Additionally, we provide a dockerised cross-compilation environment for faster compilation. With this pipeline, we aim to provide a template for programmers and makers to prototype and experiment with neural networks for real-time embedded musical applications.},
	urldate = {2024-01-29},
	author = {Pelinski, Teresa and Diaz, Rodrigo and Temprano, Adán L. Benito and McPherson, Andrew},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	file = {nime2023_22.pdf:/Users/admin/Zotero/storage/9SLYT8K4/nime2023_22.pdf:application/pdf},
}



